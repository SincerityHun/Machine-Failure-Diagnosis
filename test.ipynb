{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: pandas in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: six>=1.5 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: seaborn in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from seaborn) (3.8.3)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from seaborn) (1.26.4)\n",
      "Requirement already satisfied: pandas>=1.2 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: torch in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (12.1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: sympy in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: filelock in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: networkx in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: fsspec in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (11.0.2.54)\n",
      "Requirement already satisfied: jinja2 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (8.9.2.26)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from torch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.3.101)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: matplotlib in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (3.8.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib) (1.26.4)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib) (4.49.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: pillow>=8 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: six>=1.5 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scipy in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from scipy) (1.26.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Requirement already satisfied: scikit-learn in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from scikit-learn) (3.3.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/seonghun/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages (from scikit-learn) (1.12.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.signal import stft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU SETTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # 사용 가능한 GPU의 개수\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "    # Device 세팅\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and set as the current device.\")\n",
    "\n",
    "    # 현재 device로 설정된 GPU 확인\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(f\"Current GPU Device: index[{current_device}]\")\n",
    "    \n",
    "    # 현재 device로 설정된 GPU의 이름 출력\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(current_device)}\")\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SETTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor1 = pd.read_csv(\"raw_data/g1_sensor1.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])\n",
    "sensor2 = pd.read_csv(\"raw_data/g1_sensor2.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])\n",
    "sensor3 = pd.read_csv(\"raw_data/g1_sensor3.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])\n",
    "sensor4 = pd.read_csv(\"raw_data/g1_sensor4.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sensor 1의 데이터 크기\",sensor1.shape) # 190218개의 행, 5의 열\n",
    "print(\"sensor 1의 데이터 크기\",sensor2.shape)\n",
    "print(\"sensor 1의 데이터 크기\",sensor3.shape)\n",
    "print(\"sensor 1의 데이터 크기\",sensor4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "x_new = np.arange(0,140,0.001) # 0,0.001,0.002,,,,,,139.999\n",
    "y_new1 = []\n",
    "y_new2 = []\n",
    "y_new3 = []\n",
    "y_new4 = []\n",
    "\n",
    "# 모든 센서의 각 타입 데이터 별로 선형 보간을 수행한 결과를 추출\n",
    "# normal = 정상 데이터, type1 = 질량 불균형 고장상태, type2 = 지지불량 고장상태, type3 = 질량 불균형과 지지불량 고장 상태\n",
    "for item in [\"normal\",\"type1\",\"type2\",\"type3\"]:\n",
    "    f_linear1 = interpolate.interp1d(sensor1[\"time\"],sensor1[item],kind=\"linear\") \n",
    "    y_new1.append(f_linear1(x_new)) \n",
    "\n",
    "    f_linear2 = interpolate.interp1d(sensor2[\"time\"],sensor2[item],kind=\"linear\")\n",
    "    y_new2.append(f_linear2(x_new))\n",
    "    f_linear3 = interpolate.interp1d(sensor3[\"time\"],sensor3[item],kind=\"linear\")\n",
    "    y_new3.append(f_linear3(x_new))\n",
    "    f_linear4 = interpolate.interp1d(sensor4[\"time\"],sensor4[item],kind=\"linear\")\n",
    "    y_new4.append(f_linear4(x_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간축을 기준으로, 모든 센서에서 추출된 데이터를 이어붙인다.\n",
    "normal_ = pd.concat([sensor1[\"normal\"],sensor2[\"normal\"],sensor3[\"normal\"],sensor4[\"normal\"]],axis=1) # 각 센서에서 추출된 normal 데이터 \n",
    "type1_ = pd.concat([sensor1[\"type1\"],sensor2[\"type1\"],sensor3[\"type1\"],sensor4[\"type1\"]],axis=1) \n",
    "type2_ = pd.concat([sensor1[\"type2\"],sensor2[\"type2\"],sensor3[\"type2\"],sensor4[\"type2\"]],axis=1)\n",
    "type3_ = pd.concat([sensor1[\"type3\"],sensor2[\"type3\"],sensor3[\"type3\"],sensor4[\"type3\"]],axis=1)\n",
    "\n",
    "# 어디 센서에서 나온 결과인지, 열의 이름 달기\n",
    "normal_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
    "type1_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
    "type2_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
    "type3_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본적인 분포\n",
    "    * 주파수가 현재 normal, type1, type2, type3가 달라서, 정규 분포가 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(range(0,300),normal_[\"s1\"][:300],label=\"class=\"+str(1),marker='o',s =5) # x축: (0,300), y축: normal_[\"s1\"][:300]\n",
    "plt.scatter(range(0,300),type1_[\"s1\"][:300],label=\"class=\"+str(2),marker='o',s =5)\n",
    "plt.scatter(range(0,300),type2_[\"s1\"][:300],label=\"class=\"+str(3),marker='o',s =5)\n",
    "plt.scatter(range(0,300),type3_[\"s1\"][:300],label=\"class=\"+str(4),marker='o',s =5)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Sensor\",fontsize=15)\n",
    "plt.ylabel(\"Sensor Value\",fontsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 상관관계 확인\n",
    "    * 왠지는 모르겠지만, 안됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
    "cm = np.corrcoef(normal_.values.T)\n",
    "sns.set(font_scale=0.8)\n",
    "sns.heatmap(cm,annot=True,square=True,fmt=\".2f\",annot_kws={\"size\":10},yticklabels=names,xticklabels=names,cmap=plt.cm.Blues)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- FFT\n",
    "    * 데이터 최대 주파수 확인용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_frequency(signal, fs=0.001):\n",
    "    \"\"\"\n",
    "    신호의 최대 주파수 성분을 찾는 함수입니다.\n",
    "    \n",
    "    Parameters:\n",
    "    signal: 시간 도메인 데이터 (1차원 numpy 배열)\n",
    "    fs: 샘플링 주파수 (Hz) -> step(0.001)\n",
    "    \n",
    "    Returns:\n",
    "    max_freq: 최대 주파수 성분 (Hz)\n",
    "    magnitude_max_freq: 최대 주파수 성분의 magnitude\n",
    "    \"\"\"\n",
    "    # FFT 계산\n",
    "    n = len(signal)\n",
    "    freqs = np.fft.fftfreq(n, d=1/fs)\n",
    "    fft_values = np.fft.fft(signal)\n",
    "    \n",
    "    # Magnitude 계산\n",
    "    magnitudes = np.abs(fft_values)\n",
    "    \n",
    "    # Nyquist 주파수까지만 고려\n",
    "    half_n = n // 2\n",
    "    freqs = freqs[:half_n]\n",
    "    magnitudes = magnitudes[:half_n]\n",
    "    \n",
    "    # 최대 주파수 성분 찾기\n",
    "    max_idx = np.argmax(magnitudes)\n",
    "    max_freq = freqs[max_idx]\n",
    "    magnitude_max_freq = magnitudes[max_idx]\n",
    "    \n",
    "    return max_freq, magnitude_max_freq\n",
    "find_max_frequency(sensor1[\"normal\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- STFT\n",
    "    * 각 데이터가 어떤 주파수 분포를 띄고 있는지 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_stft(data,result_label,fs=500,nperseg=64):\n",
    "    signal = data[result_label]\n",
    "\n",
    "    noverlap = nperseg // 2\n",
    "    nfft = nperseg\n",
    "\n",
    "    # STFT\n",
    "    f,t,Zxx = stft(signal,fs=fs, nperseg = nperseg, noverlap=noverlap, nfft = nfft)\n",
    "\n",
    "    # STFT 결과를 이미지로 저장합니다.\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(t, f, np.abs(Zxx), vmin=0, vmax=np.abs(Zxx).max(), shading='auto', cmap='jet')\n",
    "\n",
    "    # x축과 y축 라벨 추가\n",
    "    plt.xlabel(\"Time (seconds)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    \n",
    "    # 컬러바 추가\n",
    "    plt.colorbar(label='Magnitude')\n",
    "\n",
    "    # 제목 추가\n",
    "    plt.title(f\"STFT Magnitude of {result_label}\")\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "# \"normal\",\"type1\",\"type2\",\"type3\"\n",
    "print(sensor1[\"normal\"])\n",
    "check_stft(sensor1,\"normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M =15 # 이동평균 필터 사이즈\n",
    "# 노이즈 제거\n",
    "# 1. normal의 이동 평균 필터 적용 + 열벡터로 데이터 재구성\n",
    "normal_s1 = np.convolve(normal_['s1'], np.ones(M), 'valid') / M; normal_s1 = normal_s1.reshape(len(normal_s1),1) \n",
    "normal_s2 = np.convolve(normal_['s2'], np.ones(M), 'valid') / M; normal_s2 = normal_s2.reshape(len(normal_s2),1)\n",
    "normal_s3 = np.convolve(normal_['s3'], np.ones(M), 'valid') / M; normal_s3 = normal_s3.reshape(len(normal_s3),1)\n",
    "normal_s4 = np.convolve(normal_['s4'], np.ones(M), 'valid') / M; normal_s4 = normal_s4.reshape(len(normal_s4),1)\n",
    "\n",
    "# 2. type1의 이동 평균 필터 적용 + 열벡터로 데이터 재구성\n",
    "type1_s1 = np.convolve(type1_['s1'], np.ones(M), 'valid') / M; type1_s1 = type1_s1.reshape(len(type1_s1),1)\n",
    "type1_s2 = np.convolve(type1_['s2'], np.ones(M), 'valid') / M; type1_s2 = type1_s2.reshape(len(type1_s2),1)\n",
    "type1_s3 = np.convolve(type1_['s3'], np.ones(M), 'valid') / M; type1_s3 = type1_s3.reshape(len(type1_s3),1)\n",
    "type1_s4 = np.convolve(type1_['s4'], np.ones(M), 'valid') / M; type1_s4 = type1_s4.reshape(len(type1_s4),1)\n",
    "\n",
    "# 3. type2의 이동 평균 필터 적용 + 열벡터로 데이터 재구성\n",
    "type2_s1 = np.convolve(type2_['s1'], np.ones(M), 'valid') / M; type2_s1 = type2_s1.reshape(len(type2_s1),1)\n",
    "type2_s2 = np.convolve(type2_['s2'], np.ones(M), 'valid') / M; type2_s2 = type2_s2.reshape(len(type2_s2),1)\n",
    "type2_s3 = np.convolve(type2_['s3'], np.ones(M), 'valid') / M; type2_s3 = type2_s3.reshape(len(type2_s3),1)\n",
    "type2_s4 = np.convolve(type2_['s4'], np.ones(M), 'valid') / M; type2_s4 = type2_s4.reshape(len(type2_s4),1)\n",
    "\n",
    "# 4. type3의 이동 평균 필터 적용 + 열벡터로 데이터 재구성\n",
    "type3_s1 = np.convolve(type3_['s1'], np.ones(M), 'valid') / M; type3_s1 = type3_s1.reshape(len(type3_s1),1)\n",
    "type3_s2 = np.convolve(type3_['s2'], np.ones(M), 'valid') / M; type3_s2 = type3_s2.reshape(len(type3_s2),1)\n",
    "type3_s3 = np.convolve(type3_['s3'], np.ones(M), 'valid') / M; type3_s3 = type3_s3.reshape(len(type3_s3),1)\n",
    "type3_s4 = np.convolve(type3_['s4'], np.ones(M), 'valid') / M; type3_s4 = type3_s4.reshape(len(type3_s4),1)\n",
    "\n",
    "# 5. 이동평균 구한거 합치기\n",
    "normal_temp = np.concatenate((normal_s1,normal_s2,normal_s3,normal_s4), axis =1)\n",
    "type1_temp = np.concatenate((type1_s1,type1_s2,type1_s3,type1_s4), axis =1)\n",
    "type2_temp = np.concatenate((type2_s1,type2_s2,type2_s3,type2_s4), axis =1)\n",
    "type3_temp = np.concatenate((type3_s1,type3_s2,type3_s3,type3_s4), axis =1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(normal_) # normal_데이터셋의 데이터 분포가 어떻게 정규화되어 있는지 학습\n",
    "\n",
    "# normal_ 데이터셋 분포에 맞게 다른 모든 데이터 셋의 분포를 전환\n",
    "normal = scaler.transform(normal_temp)\n",
    "type1 = scaler.transform(type1_temp)\n",
    "type2 = scaler.transform(type2_temp)\n",
    "type3 = scaler.transform(type3_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SPLIT ->(Train, Valid, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal)\n",
    "print('------------------------------------------------')\n",
    "print('normal data size = ', normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 끝에 NAN 쓰레기값, 초반에 불안정함때문에 중간 100,000개만 데이터로 사용\n",
    "normal = normal[30000:130000][:]\n",
    "type1 = type1[30000:130000][:]\n",
    "type2 = type2[30000:130000][:]\n",
    "type3 = type3[30000:130000][:]\n",
    "print(normal)\n",
    "print('------------------------------------------------')\n",
    "print('normal data size = ', normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분배, train = 60,000개, valid = 20,000개, test = 20,000개 \n",
    "normal_train = normal[:][:60000]; normal_valid = normal[:][60000:80000]; normal_test =normal[:][80000:]\n",
    "type1_train = type1[:][:60000]; type1_valid = type1[:][60000:80000]; type1_test =type1[:][80000:]\n",
    "type2_train = type2[:][:60000]; type2_valid = type2[:][60000:80000]; type2_test =type2[:][80000:]\n",
    "type3_train = type3[:][:60000]; type3_valid = type3[:][60000:80000]; type3_test =type3[:][80000:]\n",
    "\n",
    "# 데이터 합치기\n",
    "train = np.concatenate((normal_train,type1_train,type2_train,type3_train))\n",
    "valid = np.concatenate((normal_valid,type1_valid,type2_valid,type3_valid))\n",
    "test = np.concatenate((normal_test,type1_test,type2_test,type3_test))\n",
    "print(\"train data의 형태:\", train.shape) # normal_train -> type1_train -> type2_train -> type3_train: 같은 분포\n",
    "print(\"valid data의 형태:\", valid.shape)\n",
    "print(\" test data의 형태:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 예측한 결과값을 담을 데이터 구조 생성\n",
    "train_label = np.concatenate((np.full((60000,1),0), np.full((60000,1),1),\n",
    "np.full((60000,1),2), np.full((60000,1),3)))\n",
    "valid_label = np.concatenate((np.full((20000,1),0), np.full((20000,1),1),\n",
    "np.full((20000,1),2), np.full((20000,1),3)))\n",
    "test_label = np.concatenate((np.full((20000,1),0), np.full((20000,1),1),\n",
    "np.full((20000,1),2), np.full((20000,1),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data, valid data, test data 전부 index 셔플\n",
    "idx = np.arange(train.shape[0]); np.random.shuffle(idx);\n",
    "train = train[:][idx]; train_label = train_label[:][idx]\n",
    "idx_v = np.arange(valid.shape[0]); np.random.shuffle(idx_v);\n",
    "valid = valid[:][idx_v]; valid_label = valid_label[:][idx_v]\n",
    "idx_t = np.arange(test.shape[0]); np.random.shuffle(idx_t);\n",
    "test = test[:][idx_t]; test_label = test_label[:][idx_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토치 텐서로 변환, 그냥 좀 절삭\n",
    "x_train = torch.from_numpy(train).float()\n",
    "y_train = torch.from_numpy(train_label).float().T[0]\n",
    "x_valid = torch.from_numpy(valid).float()\n",
    "y_valid = torch.from_numpy(valid_label).float().T[0]\n",
    "x_test = torch.from_numpy(test).float()\n",
    "y_test = torch.from_numpy(test_label).float().T[0]\n",
    "print(\"변경 전\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"변경 후\")\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# 데이터셋 생성 및 배치사이즈로 미리 나누며 iterator 생성\n",
    "BATCH_SIZE = 5000\n",
    "train = TensorDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(train, batch_size =BATCH_SIZE, shuffle=True)\n",
    "valid = TensorDataset(x_valid, y_valid)\n",
    "valid_dataloader = DataLoader(valid, batch_size =len(x_valid), shuffle=False)\n",
    "test = TensorDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(test, batch_size =len(x_valid), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET AI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAMP_DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAMP_DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features =4, out_features =100)\n",
    "        self.layer2 = nn.Linear(in_features =100, out_features =100)\n",
    "        self.layer3 = nn.Linear(in_features =100, out_features =100)\n",
    "        self.layer4 = nn.Linear(in_features =100, out_features =4)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, input):\n",
    "        out =self.layer1(input)\n",
    "        out =self.relu(out)\n",
    "        out =self.dropout(out)\n",
    "\n",
    "        out =self.layer2(out)\n",
    "        out =self.relu(out)\n",
    "        out =self.dropout(out)\n",
    "\n",
    "        out =self.layer3(out)\n",
    "        out =self.relu(out)\n",
    "        out =self.dropout(out)\n",
    "\n",
    "        out =self.layer4(out)\n",
    "        return out\n",
    "\n",
    "model_check = KAMP_DNN()\n",
    "print(model_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAMP_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAMP_CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=1, out_channels=100, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=100, out_channels=100, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=100, out_channels=100, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=100, out_channels=4, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(4),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1))\n",
    "\n",
    "        self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear = nn.Linear(4, 4)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(1)\n",
    "        out =self.conv1(input)\n",
    "        # out = self.conv2(out)\n",
    "        # out = self.conv3(out)\n",
    "        out =self.conv4(out)\n",
    "        out =self.final_pool(out)\n",
    "        out =self.linear(out.squeeze(-1))\n",
    "        return out\n",
    "model_check = KAMP_CNN()\n",
    "print(model_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------------------------------------------------------')\n",
    "input = torch.tensor([[[0.0, 6.0, 9.0, 1.0]]])\n",
    "print('\"input is same below.\"')\n",
    "print(input)\n",
    "print('--------------------------------------------------------------------')\n",
    "model = nn.Conv1d(1, 4, 2, bias =False)\n",
    "model.weight.data = torch.zeros(model.weight.data.size())\n",
    "model.weight.data[:, :, :2] =1\n",
    "print('\"kernal is same below.\"')\n",
    "print(model.weight.data)\n",
    "print('--------------------------------------------------------------------')\n",
    "output = model(input)\n",
    "print('\"output is same below (without bias).\"')\n",
    "print(output)\n",
    "print('--------------------------------------------------------------------')\n",
    "model1 = nn.Conv1d(1, 4, 2)\n",
    "model1.weight.data = torch.zeros(model1.weight.data.size())\n",
    "model1.weight.data[:, :, :2] =1\n",
    "output = model1(input)\n",
    "print('\"output is same below (with bias).\"')\n",
    "print(output)\n",
    "print('--------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class KAMP_RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAMP_RNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size =4, hidden_size =100, num_layers =2,\n",
    "        batch_first=True, dropout =0.2)\n",
    "        self.fc = nn.Linear(in_features =100, out_features =4)\n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(1)\n",
    "        out, _ =self.lstm(input)\n",
    "        out = out.view(-1,100)\n",
    "        output =self.fc(out)\n",
    "        return output\n",
    "\n",
    "model_check = KAMP_RNN()\n",
    "print(model_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU: device\n",
    "def train_model(model, criterion, optimizer, num_epoch, train_dataloader, PATH):\n",
    " # Model을 GPU로 이동\n",
    " model.to(device)\n",
    "\n",
    " \n",
    " loss_values = []\n",
    " loss_values_v = [] \n",
    " accuracy_past =0\n",
    " for epoch in range(1, num_epochs +1):\n",
    "    #---------------------- 모델 학습 ---------------------#\n",
    "    model.train()\n",
    "    batch_number =0\n",
    "    running_loss =0.0\n",
    "    for batch_idx, samples in enumerate(train_dataloader):\n",
    "        # 데이터 GPU로 옮기기\n",
    "        x_train, y_train = samples[0].to(device), samples[1].to(device) \n",
    "\n",
    "        # 변수 초기화\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model.forward(x_train)\n",
    "        loss = criterion(y_hat,y_train.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        batch_number +=1\n",
    "\n",
    "    loss_values.append(running_loss / batch_number)\n",
    " #---------------------- 모델 검증 ---------------------#\n",
    "    model.eval()\n",
    "    accuracy =0.0\n",
    "    total =0.0\n",
    "    for batch_idx, data in enumerate(valid_dataloader):\n",
    "        x_valid, y_valid = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        v_hat = model.forward(x_valid)\n",
    "        v_loss = criterion(v_hat,y_valid.long())\n",
    "        _, predicted = torch.max(v_hat.data, 1)\n",
    "        total += y_valid.size(0)\n",
    "        accuracy += (predicted == y_valid).sum().item()\n",
    "    loss_values_v.append(loss.item())\n",
    "    accuracy = (accuracy / total)\n",
    " #----------------Check for early stopping---------------#\n",
    "    if epoch % 1 ==0:\n",
    "        print('[Epoch {}/{}] [Train_Loss: {:.6f} /Valid_Loss: {:.6f}]'.format(epoch, num_epochs, loss.item(),v_loss.item()))\n",
    "        print('[Epoch {}/{}] [Accuracy : {:.6f}]'.format(epoch, num_epochs, accuracy))\n",
    "    \n",
    "    # checkpoint + early stopping\n",
    "    if accuracy_past < accuracy:\n",
    "        accuracy_past = accuracy\n",
    "        torch.save(model.state_dict(), PATH + f'model_epoch_{epoch}_acc_{accuracy:.4f}.pt')\n",
    "        print(f\"Checkpoint saved at epoch {epoch} with validation accuracy {accuracy:.4f}.\")\n",
    "\n",
    "# return loss..\n",
    " return loss_values, loss_values_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model = KAMP_CNN()\n",
    "num_epochs =1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(CNN_model.parameters())\n",
    "PATH ='save/CNN/'\n",
    "CNN_loss_values, CNN_loss_values_v = train_model(CNN_model, criterion, optimizer,\n",
    "num_epochs, train_dataloader, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = KAMP_RNN()\n",
    "num_epochs =1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(RNN_model.parameters())\n",
    "PATH ='save/RNN/'\n",
    "RNN_loss_values, RNN_loss_values_v = train_model(RNN_model, criterion, optimizer,\n",
    "num_epochs, train_dataloader, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, PATH):\n",
    "    model = torch.load(PATH +'model.pt')\n",
    "    #---------------------- 모델 시험 ---------------------#\n",
    "    model.eval()\n",
    "    total =0.0\n",
    "    accuracy =0.0\n",
    "    for batch_idx, data in enumerate(test_dataloader):\n",
    "        x_test, y_test = data[0].to(device),data[1].to(device)\n",
    "\n",
    "        t_hat = model(x_test)\n",
    "        _, predicted = torch.max(t_hat.data, 1)\n",
    "        total += y_test.size(0)\n",
    "        accuracy += (predicted == y_test).sum().item()\n",
    "    accuracy = (accuracy / total)\n",
    "    #------------------------------------------------------#\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def draw_confusion_matrix(model, xt, yt, PATH):\n",
    "    y_pred = []; y_true = []\n",
    "    # 1. 모델 예측 결과 뽑기\n",
    "    model.eval()\n",
    "    y_hat = model(xt) # y_hat은 모델로 예측한 결과\n",
    "    output = (torch.max(torch.exp(y_hat), 1)[1]).data.cpu().numpy() # 결과값만 추출 \n",
    "    y_pred.extend(output)\n",
    "\n",
    "    # 2. 실제 정답값 뽑기\n",
    "    labels = y_test.data.cpu().numpy()\n",
    "    y_true.extend(labels)\n",
    "    # 분류 항목\n",
    "    classes = ('Normal', 'Type1', 'Type2', 'Type3')\n",
    "\n",
    "    # Confussion Matrix 생성\n",
    "    plt.figure(figsize = (7,5))\n",
    "    dlen = float(len(x_test)) # test data 크기 : 여기서는 80000\n",
    "    cm = confusion_matrix(y_true, y_pred) \n",
    "\n",
    "    df_cm = pd.DataFrame(cm/dlen, index = [i for i in classes],columns = [i for i in classes])\n",
    "    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\", size=24, fontweight='bold')\n",
    "    plt.xlabel(\"Predicted Label\", size=16); plt.ylabel(\"Actual Label\", size=16)\n",
    "    plt.rc('xtick', labelsize=12); plt.rc('ytick', labelsize=12); plt.yticks(rotation=0)\n",
    "    plt.savefig(PATH +'cm_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_graph(loss_values, loss_values_v):\n",
    "    plt.figure()\n",
    "    plt.plot(loss_values)\n",
    "    plt.plot(loss_values_v)\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.ylabel(\"loss\", fontsize=\"large\")\n",
    "    plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "    plt.legend([\"train\", \"validation\"])\n",
    "    plt.tight_layout()\n",
    "    # 결과 저장\n",
    "    plt.savefig(PATH +'lossplot_output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(CNN_model,'save/CNN/')\n",
    "# test_model(RNN_model,'save/RNN/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confusion_matrix(CNN_model, x_test, y_test, 'save/CNN/')\n",
    "draw_confusion_matrix(RNN_model, x_test, y_test, 'save/RNN/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_graph(CNN_loss_values, CNN_loss_values_v)\n",
    "plot_loss_graph(RNN_loss_values, RNN_loss_values_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-job",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

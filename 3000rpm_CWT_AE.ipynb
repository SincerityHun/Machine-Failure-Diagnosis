{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 계획\n",
    "1. Import\n",
    "2. Data Processing (Preprocessing)\n",
    "    - Data Load\n",
    "    - Original Signal + Visualization\n",
    "    - CWT\n",
    "3. Model Define (Pytorch) + Hyperparameter Tuning\n",
    "    - LSTM Autoencoder\n",
    "        * batch_size\n",
    "        * hidden_size\n",
    "        * layer_numbers\n",
    "        * num_epochs\n",
    "        * learning_rate\n",
    "4. Training\n",
    "5. Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.fft import fft, ifft\n",
    "import pywt\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Processing\n"
     ]
    }
   ],
   "source": [
    "# Data Processing\n",
    "print(\"Data Processing\")\n",
    "RPM = 3000\n",
    "base_path = \"5000hz_raw_data/\" + str(RPM) + \"rpm/\"\n",
    "folders = [\n",
    "    str(RPM) + \"rpm \" + \"normal data\",\n",
    "    str(RPM) + \"rpm \" + \"carriage damage\",\n",
    "    str(RPM) + \"rpm \" + \"high-speed damage\",\n",
    "    str(RPM) + \"rpm \" + \"lack of lubrication\",\n",
    "    str(RPM) + \"rpm \" + \"oxidation and corrosion\",\n",
    "]\n",
    "columns = [\"motor1_x\", \"motor1_y\", \"motor1_z\", \"sound\", \"time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 읽고 결합하는 함수\n",
    "def read_and_concatenate(folder):\n",
    "    all_files = []\n",
    "    for file_name in os.listdir(folder):\n",
    "        if file_name.endswith(\".csv\"):\n",
    "            file_path = os.path.join(folder, file_name)\n",
    "            df = pd.read_csv(file_path, usecols=columns)\n",
    "            all_files.append(df)\n",
    "            break  # TEST 한개의 파일만 사용\n",
    "    combined_df = pd.concat(all_files)\n",
    "    combined_df.sort_values(\"time\", inplace=True)  # 시간 열 기준 정렬\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CWT를 적용하는 함수\n",
    "def apply_cwt(data, scales, wavelet_name=\"morl\"):\n",
    "    coefficients, frequencies = pywt.cwt(data, scales, wavelet_name)\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CWT 변환\n",
    "def apply_cwt_to_dataset(data, scales, wavelet_name=\"morl\"):\n",
    "    cwt_features = []\n",
    "    for feature in data.T:  # Apply CWT on each feature (column) of the dataset\n",
    "        cwt_matrix, _ = pywt.cwt(feature, scales, wavelet_name)\n",
    "        # Normalize the CWT matrix\n",
    "        cwt_matrix = (cwt_matrix - np.mean(cwt_matrix)) / np.std(cwt_matrix)\n",
    "        cwt_features.append(cwt_matrix)\n",
    "    # Stack to form [samples, features, time, CWT_coefficients]\n",
    "    return np.stack(cwt_features, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df = dict()\n",
    "folder_index = [\n",
    "    \"normal data\",\n",
    "    \"carriage damage\",\n",
    "    \"high-speed damage\",\n",
    "    \"lack of lubrication\",\n",
    "    \"oxidation and corrosion\",\n",
    "]\n",
    "# 각 폴더에서 데이터를 처리\n",
    "for index, folder_name in enumerate(folders):\n",
    "    folder_path = os.path.join(base_path, folder_name)\n",
    "    concatenated_df[folder_index[index]] = read_and_concatenate(folder_path)\n",
    "\n",
    "    # time 열 제거\n",
    "    concatenated_df[folder_index[index]].drop(columns=\"time\", inplace=True)\n",
    "    # Label 열 추가\n",
    "    concatenated_df[folder_index[index]][\"label\"] = index\n",
    "\n",
    "# 데이터 결합\n",
    "combined_data = pd.concat(\n",
    "    [\n",
    "        concatenated_df[folder_index[0]],\n",
    "        concatenated_df[folder_index[1]],\n",
    "        concatenated_df[folder_index[2]],\n",
    "        concatenated_df[folder_index[3]],\n",
    "        concatenated_df[folder_index[4]],\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "features = combined_data[[\"motor1_x\", \"motor1_y\", \"motor1_z\", \"sound\"]]\n",
    "labels = combined_data[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정규화\n",
    "scalser = StandardScaler()\n",
    "X_scaled = scalser.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "# 데이터를 훈련 및 테스트 세트로 분할\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features.values, labels.values, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 훈련 데이터를 훈련 및 검증 세트로 분할\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CWT 변환\n"
     ]
    }
   ],
   "source": [
    "print(\"CWT 변환\")\n",
    "scales = np.arange(1, 128)  # Example range of scales\n",
    "X_train_cwt = apply_cwt_to_dataset(X_train, scales)\n",
    "X_val_cwt = apply_cwt_to_dataset(X_val, scales)\n",
    "X_test_cwt = apply_cwt_to_dataset(X_test, scales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_cwt, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_cwt, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_cwt, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_tensor shape:  torch.Size([127, 4, 960000])\n",
      "y_train_tensor shape:  torch.Size([960000])\n",
      "X_val_tensor shape:  torch.Size([127, 4, 240000])\n",
      "y_val_tensor shape:  torch.Size([240000])\n",
      "X_test_tensor shape:  torch.Size([127, 4, 300000])\n",
      "y_test_tensor shape:  torch.Size([300000])\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train_tensor shape: \", X_train_tensor.shape)\n",
    "print(\"y_train_tensor shape: \", y_train_tensor.shape)\n",
    "print(\"X_val_tensor shape: \", X_val_tensor.shape)\n",
    "print(\"y_val_tensor shape: \", y_val_tensor.shape)\n",
    "print(\"X_test_tensor shape: \", X_test_tensor.shape)\n",
    "print(\"y_test_tensor shape: \", y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch의 Dataset 및 DataLoader 생성\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, latent_dim):\n",
    "        super(LSTMAutoencoder, self).__init__()\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.latent = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.decoder = nn.LSTM(latent_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.output_layer = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc_output, _ = self.encoder(x)\n",
    "        latent = self.latent(enc_output[:, -1, :])\n",
    "        latent = latent.unsqueeze(1).repeat(1, x.size(1), 1)\n",
    "        dec_output, _ = self.decoder(latent)\n",
    "        output = self.output_layer(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 분류기\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_dim = X_train.shape[2]  # [\"motor1_x\", \"motor1_y\", \"motor1_z\", \"sound\"]\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "latent_dim = 64\n",
    "hidden_dim_classifier = 64\n",
    "num_classes = len(np.unique(labels.values))  # Number of unique labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 초기화\n",
    "model = LSTMAutoencoder(input_dim, hidden_dim, num_layers, latent_dim)\n",
    "classifier = Classifier(latent_dim, hidden_dim_classifier, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function 및 optimizer 정의\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion_classifier = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 파일 경로 설정\n",
    "checkpoint_path = \"./checkpoint/cwt_lstm_autoencoder/model_checkpoint\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    classifier.train()\n",
    "    train_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for batch_x, batch_y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_x)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        # Use the encoder to get the latent representation\n",
    "        enc_output, _ = model.encoder(batch_x)\n",
    "        latent = model.latent(enc_output[:, -1, :])\n",
    "        # Use the classifier to predict the class\n",
    "        output_classifier = classifier(latent)\n",
    "        loss_classifier = criterion_classifier(output_classifier, batch_y)\n",
    "        loss_classifier.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 정확도 계산\n",
    "        _, predicted = torch.max(output_classifier, 1)\n",
    "        total_train += batch_y.size(0)\n",
    "        correct_train += (predicted == batch_y).sum().item()\n",
    "\n",
    "    # Train Loss & Accuracy 계산\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # 검증 세트로 모델 평가\n",
    "    model.eval()\n",
    "    classifier.eval()\n",
    "    val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in val_loader:\n",
    "            output = model(batch_x)\n",
    "            loss = criterion(output, batch_x)\n",
    "            val_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "            # Use the encoder to get the latent representation\n",
    "            enc_output, _ = model.encoder(batch_x)\n",
    "            latent = model.latent(enc_output[:, -1, :])\n",
    "            # Use the classifier to predict the class\n",
    "            output_classifier = classifier(latent)\n",
    "\n",
    "            # 정확도 계산\n",
    "            _, predicted = torch.max(output_classifier, 1)\n",
    "            total_val += batch_y.size(0)\n",
    "            correct_val += (predicted == batch_y).sum().item()\n",
    "\n",
    "    # Validation Loss & Accuracy 계산\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    val_accuracy = correct_val / total_val\n",
    "\n",
    "    # 모델의 체크포인트 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        # 체크포인트 저장\n",
    "        torch.save(\n",
    "            {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"classifier_state_dict\": classifier.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            },\n",
    "            checkpoint_path + str(epoch) + \".pth\",\n",
    "        )\n",
    "        print(\"Checkpoint saved.\")\n",
    "\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(\n",
    "            f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Training Accuracy: {train_accuracy:.4f}, Validation Accuracy: {val_accuracy:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "# 체크포인트 불러오기\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "classifier.load_state_dict(checkpoint[\"classifier_state_dict\"])\n",
    "\n",
    "model.eval()\n",
    "classifier.eval()\n",
    "test_loss = 0.0\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for batch_x, batch_y in tqdm(test_loader, desc=\"Testing\"):\n",
    "        output = model(batch_x)\n",
    "        loss = criterion(output, batch_x)\n",
    "        test_loss += loss.item() * batch_x.size(0)\n",
    "\n",
    "        # Use the encoder to get the latent representation\n",
    "        enc_output, _ = model.encoder(batch_x)\n",
    "        latent = model.latent(enc_output[:, -1, :])\n",
    "        # Use the classifier to predict the class\n",
    "        output_classifier = classifier(latent)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(output_classifier, 1)\n",
    "        total_test += batch_y.size(0)\n",
    "        correct_test += (predicted == batch_y).sum().item()\n",
    "\n",
    "# Test Loss & Accuracy 계산\n",
    "test_loss /= len(test_loader.dataset)\n",
    "test_accuracy = correct_test / total_test\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-job",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

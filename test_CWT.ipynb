{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install scikit-learn\n",
    "!pip install pyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.fft import fft, ifft\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU SETTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available, using CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # 사용 가능한 GPU의 개수\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "    # Device 세팅\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and set as the current device.\")\n",
    "\n",
    "    # 현재 device로 설정된 GPU 확인\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(f\"Current GPU Device: index[{current_device}]\")\n",
    "    \n",
    "    # 현재 device로 설정된 GPU의 이름 출력\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(current_device)}\")\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SETTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor1 = pd.read_csv(\"raw_data/g1_sensor1.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])\n",
    "sensor2 = pd.read_csv(\"raw_data/g1_sensor2.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])\n",
    "sensor3 = pd.read_csv(\"raw_data/g1_sensor3.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])\n",
    "sensor4 = pd.read_csv(\"raw_data/g1_sensor4.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensor 1의 데이터 크기 (190218, 5)\n",
      "sensor 1의 데이터 크기 (184211, 5)\n",
      "sensor 1의 데이터 크기 (196079, 5)\n",
      "sensor 1의 데이터 크기 (183969, 5)\n"
     ]
    }
   ],
   "source": [
    "print(\"sensor 1의 데이터 크기\",sensor1.shape) # 190218개의 행, 5의 열\n",
    "print(\"sensor 1의 데이터 크기\",sensor2.shape)\n",
    "print(\"sensor 1의 데이터 크기\",sensor3.shape)\n",
    "print(\"sensor 1의 데이터 크기\",sensor4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "x_new = np.arange(0,140,0.001) # 0,0.001,0.002,,,,,,139.999\n",
    "y_new1 = []\n",
    "y_new2 = []\n",
    "y_new3 = []\n",
    "y_new4 = []\n",
    "\n",
    "# 모든 센서의 각 타입 데이터 별로 선형 보간을 수행한 결과를 추출\n",
    "for item in [\"normal\",\"type1\",\"type2\",\"type3\"]:\n",
    "    f_linear1 = interpolate.interp1d(sensor1[\"time\"],sensor1[item],kind=\"linear\") \n",
    "    y_new1.append(f_linear1(x_new)) \n",
    "\n",
    "    f_linear2 = interpolate.interp1d(sensor2[\"time\"],sensor2[item],kind=\"linear\")\n",
    "    y_new2.append(f_linear2(x_new))\n",
    "    f_linear3 = interpolate.interp1d(sensor3[\"time\"],sensor3[item],kind=\"linear\")\n",
    "    y_new3.append(f_linear3(x_new))\n",
    "    f_linear4 = interpolate.interp1d(sensor4[\"time\"],sensor4[item],kind=\"linear\")\n",
    "    y_new4.append(f_linear4(x_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>normal</th>\n",
       "      <th>type1</th>\n",
       "      <th>type2</th>\n",
       "      <th>type3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.853307</td>\n",
       "      <td>-3.464579</td>\n",
       "      <td>0.555219</td>\n",
       "      <td>3.919664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000736</td>\n",
       "      <td>-0.740463</td>\n",
       "      <td>-2.448986</td>\n",
       "      <td>-0.234687</td>\n",
       "      <td>4.145351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001472</td>\n",
       "      <td>-0.138630</td>\n",
       "      <td>-1.922383</td>\n",
       "      <td>-0.009000</td>\n",
       "      <td>2.941685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.049443</td>\n",
       "      <td>-0.906790</td>\n",
       "      <td>-0.272301</td>\n",
       "      <td>2.603155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.002944</td>\n",
       "      <td>-0.289088</td>\n",
       "      <td>-0.568259</td>\n",
       "      <td>-0.986978</td>\n",
       "      <td>1.361874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190213</th>\n",
       "      <td>139.996768</td>\n",
       "      <td>-1.156354</td>\n",
       "      <td>-2.696750</td>\n",
       "      <td>0.844491</td>\n",
       "      <td>-2.109427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190214</th>\n",
       "      <td>139.997504</td>\n",
       "      <td>-0.930666</td>\n",
       "      <td>-1.380241</td>\n",
       "      <td>0.919720</td>\n",
       "      <td>-2.222270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190215</th>\n",
       "      <td>139.998240</td>\n",
       "      <td>-0.554521</td>\n",
       "      <td>-2.132532</td>\n",
       "      <td>0.731647</td>\n",
       "      <td>-2.109427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190216</th>\n",
       "      <td>139.998976</td>\n",
       "      <td>-1.419655</td>\n",
       "      <td>-2.433448</td>\n",
       "      <td>1.183022</td>\n",
       "      <td>-3.087405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190217</th>\n",
       "      <td>139.999712</td>\n",
       "      <td>-0.441677</td>\n",
       "      <td>-2.245376</td>\n",
       "      <td>1.333480</td>\n",
       "      <td>-2.184656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>190218 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              time    normal     type1     type2     type3\n",
       "0         0.000000 -0.853307 -3.464579  0.555219  3.919664\n",
       "1         0.000736 -0.740463 -2.448986 -0.234687  4.145351\n",
       "2         0.001472 -0.138630 -1.922383 -0.009000  2.941685\n",
       "3         0.002208  0.049443 -0.906790 -0.272301  2.603155\n",
       "4         0.002944 -0.289088 -0.568259 -0.986978  1.361874\n",
       "...            ...       ...       ...       ...       ...\n",
       "190213  139.996768 -1.156354 -2.696750  0.844491 -2.109427\n",
       "190214  139.997504 -0.930666 -1.380241  0.919720 -2.222270\n",
       "190215  139.998240 -0.554521 -2.132532  0.731647 -2.109427\n",
       "190216  139.998976 -1.419655 -2.433448  1.183022 -3.087405\n",
       "190217  139.999712 -0.441677 -2.245376  1.333480 -2.184656\n",
       "\n",
       "[190218 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간축을 기준으로, 모든 센서에서 추출된 데이터를 이어붙인다.\n",
    "normal_ = pd.concat([sensor1[\"normal\"],sensor2[\"normal\"],sensor3[\"normal\"],sensor4[\"normal\"]],axis=1) # 각 센서에서 추출된 normal 데이터 \n",
    "type1_ = pd.concat([sensor1[\"type1\"],sensor2[\"type1\"],sensor3[\"type1\"],sensor4[\"type1\"]],axis=1) # Type 1 이상치 데이터\n",
    "type2_ = pd.concat([sensor1[\"type2\"],sensor2[\"type2\"],sensor3[\"type2\"],sensor4[\"type2\"]],axis=1) # Type 2 이상치 데이터\n",
    "type3_ = pd.concat([sensor1[\"type3\"],sensor2[\"type3\"],sensor3[\"type3\"],sensor4[\"type3\"]],axis=1) # Type 3 이상치 데이터\n",
    "\n",
    "# 어디 센서에서 나온 결과인지, 열의 이름 달기\n",
    "normal_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
    "type1_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
    "type2_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
    "type3_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.853307</td>\n",
       "      <td>0.048823</td>\n",
       "      <td>-0.437626</td>\n",
       "      <td>-1.116226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.740463</td>\n",
       "      <td>-0.029477</td>\n",
       "      <td>-0.437626</td>\n",
       "      <td>-0.379672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.138630</td>\n",
       "      <td>-0.029477</td>\n",
       "      <td>0.280889</td>\n",
       "      <td>-1.271290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.049443</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.280889</td>\n",
       "      <td>-0.612268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.289088</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>-0.257997</td>\n",
       "      <td>-0.689800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196074</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.257427</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196075</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.146738</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196076</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.371273</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196077</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.416181</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196078</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.257427</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>196079 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              s1        s2        s3        s4\n",
       "0      -0.853307  0.048823 -0.437626 -1.116226\n",
       "1      -0.740463 -0.029477 -0.437626 -0.379672\n",
       "2      -0.138630 -0.029477  0.280889 -1.271290\n",
       "3       0.049443  0.009673  0.280889 -0.612268\n",
       "4      -0.289088  0.009673 -0.257997 -0.689800\n",
       "...          ...       ...       ...       ...\n",
       "196074       NaN       NaN -0.257427       NaN\n",
       "196075       NaN       NaN  0.146738       NaN\n",
       "196076       NaN       NaN  0.371273       NaN\n",
       "196077       NaN       NaN  0.416181       NaN\n",
       "196078       NaN       NaN -0.257427       NaN\n",
       "\n",
       "[196079 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normal_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본적인 분포\n",
    "    * 주파수가 현재 normal, type1, type2, type3가 달라서, 정규 분포가 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(range(0,300),normal_[\"s1\"][:300],label=\"class=\"+str(1),marker='o',s =5) # x축: (0,300), y축: normal_[\"s1\"][:300]\n",
    "# plt.scatter(range(0,300),type1_[\"s1\"][:300],label=\"class=\"+str(2),marker='o',s =5)\n",
    "# plt.scatter(range(0,300),type2_[\"s1\"][:300],label=\"class=\"+str(3),marker='o',s =5)\n",
    "# plt.scatter(range(0,300),type3_[\"s1\"][:300],label=\"class=\"+str(4),marker='o',s =5)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Sensor\",fontsize=15)\n",
    "plt.ylabel(\"Sensor Value\",fontsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주어진 주파수를 Morlet 웨이블릿을 활용해 CWT변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웨이블릿 생성 코드\n",
    "def psi(T, f0=6):\n",
    "    '''\n",
    "\t  T : parameter for adjusting length of wavelet\n",
    "\t  f0 : parameter for time-frequenct resolution trade off \n",
    "    '''\n",
    "    x = np.linspace(-2 * np.pi, 2 * np.pi, T)\n",
    "    return (np.pi ** -0.25) * np.exp(1j * f0 * x - x ** 2 / 2)\n",
    "\n",
    "# 특정 웨이블릿과 주어진 신호 사이의 상관계수 Convolution 연산 코드\n",
    "def wavelet_convolution(tup):\n",
    "    f = tup[0]\n",
    "    T = tup[1]\n",
    "    f_len = np.shape(f)[0]\n",
    "    f_hat = np.append(f, np.zeros(T))\n",
    "    h = psi(T)\n",
    "    h_hat = np.append(h, np.zeros(f_len))\n",
    "    return ifft(fft(f_hat)*fft(h_hat))[round(T/2) : round(T/2) + f_len]\n",
    "\n",
    "# 전체 시간의 CWT 출력\n",
    "def cwt(f, t0 = 20):\n",
    "    '''\n",
    "    f : input signal\n",
    "    t0 : minimum length of wavelet\n",
    "    '''\n",
    "    f_len = np.shape(f)[0]\n",
    "    result = np.array(list(map(wavelet_convolution, [(f, x) for x in range(t0, f_len, 10)])))\n",
    "    return result\n",
    "\n",
    "# 기본 Input 시그널 확인\n",
    "input_signal = normal_[\"s1\"]\n",
    "print(np.shape(input_signal))\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(range(0, 300), input_signal[:300], label=\"class=\" + str(1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리를 활용한 CWT 변환\n",
    "wavelet = 'cmor'\n",
    "scales = np.arange(1,64) # 스케일이 커질 수록, 낮은 주파수 성분을 잡아낼 수 있음\n",
    "coefficients, frequencies = pywt.cwt(input_signal, scales, wavelet)\n",
    "# CWT 결과를 시각화합니다.\n",
    "print(np.abs(coefficients))\n",
    "plt.imshow(np.abs(coefficients), extent=[0, len(input_signal), frequencies[-1], frequencies[0]], aspect='auto')\n",
    "plt.yscale('log')\n",
    "plt.colorbar(label='Magnitude')\n",
    "plt.title('Continuous Wavelet Transform (CWT)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.xlabel('Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWT 변환 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m scales \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m128\u001b[39m) \u001b[38;5;66;03m# 스케일이 커질 수록, 낮은 주파수 성분을 잡아낼 수 있음-> 이거 그냥 128이 제일 잘 나와서 이걸로 함\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# 각 데이터셋에 대해 CWT 변환 적용\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m normal_cwt \u001b[38;5;241m=\u001b[39m \u001b[43mapply_cwt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormal_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\u001b[43mwavelet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m type1_cwt \u001b[38;5;241m=\u001b[39m apply_cwt(type1_, scales,wavelet)\n\u001b[1;32m     23\u001b[0m type2_cwt \u001b[38;5;241m=\u001b[39m apply_cwt(type2_, scales,wavelet)\n",
      "Cell \u001b[0;32mIn[10], line 9\u001b[0m, in \u001b[0;36mapply_cwt\u001b[0;34m(data, scales, wavelet)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData for sensor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not 1-dimensional.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# 연속 웨이블릿 변환 적용\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m cwt_matrix, frequencies \u001b[38;5;241m=\u001b[39m \u001b[43mpywt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcwt\u001b[49m\u001b[43m(\u001b[49m\u001b[43msensor_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscales\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwavelet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 해당 시계열의 cwt_matrix의 절댓값이 가장 큰 값의 frequencies을 넣어준다.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Find the index of the max coefficient at each time point across all scales\u001b[39;00m\n\u001b[1;32m     12\u001b[0m cwt_coeffs\u001b[38;5;241m.\u001b[39mappend(np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39mabs(cwt_matrix),axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))  \u001b[38;5;66;03m# 결과를 1차원으로 평탄\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages/pywt/_cwt.py:165\u001b[0m, in \u001b[0;36mcwt\u001b[0;34m(data, scales, wavelet, sampling_period, method, axis)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 165\u001b[0m         conv \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_psi_scale\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;66;03m# batch convolution via loop\u001b[39;00m\n\u001b[1;32m    168\u001b[0m         conv_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.11/envs/ml-job/lib/python3.10/site-packages/numpy/core/numeric.py:834\u001b[0m, in \u001b[0;36mconvolve\u001b[0;34m(a, v, mode)\u001b[0m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(v) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv cannot be empty\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmultiarray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrelate\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# apply_cwt 함수 정의: 각 센서 데이터에 대해 CWT를 적용합니다.\n",
    "def apply_cwt(data, scales, wavelet='cmor'):\n",
    "    cwt_coeffs = []\n",
    "    for column in data:\n",
    "        sensor_data = data[column].values  # Pandas Series를 numpy 배열로 변환\n",
    "        if sensor_data.ndim != 1:\n",
    "            raise ValueError(f\"Data for sensor {column} is not 1-dimensional.\")\n",
    "        # 연속 웨이블릿 변환 적용\n",
    "        cwt_matrix, frequencies = pywt.cwt(sensor_data, scales, wavelet)\n",
    "        # 해당 시계열의 cwt_matrix의 절댓값이 가장 큰 값의 frequencies을 넣어준다.\n",
    "        # Find the index of the max coefficient at each time point across all scales\n",
    "        cwt_coeffs.append(np.mean(np.abs(cwt_matrix),axis=0))  # 결과를 1차원으로 평탄\n",
    "        # cwt_coeffs.append(np.abs(cwt_matrix))  # 결과를 1차원으로 평탄\n",
    "    # numpy 배열로 변환\n",
    "    return np.column_stack(cwt_coeffs)\n",
    "\n",
    "wavelet = 'morl'\n",
    "scales = np.arange(1,128) # 스케일이 커질 수록, 낮은 주파수 성분을 잡아낼 수 있음-> 이거 그냥 128이 제일 잘 나와서 이걸로 함\n",
    "\n",
    "# 각 데이터셋에 대해 CWT 변환 적용\n",
    "normal_cwt = apply_cwt(normal_, scales,wavelet)\n",
    "type1_cwt = apply_cwt(type1_, scales,wavelet)\n",
    "type2_cwt = apply_cwt(type2_, scales,wavelet)\n",
    "type3_cwt = apply_cwt(type3_, scales,wavelet)\n",
    "\n",
    "# CWT 결과를 시각화하는 함수 정의\n",
    "def visualize_cwt(coefficients, sensor_index, scales):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(np.abs(coefficients[sensor_index]), extent=[0, len(coefficients[sensor_index][0]), scales[-1], scales[0]], aspect='auto', cmap='viridis')\n",
    "    plt.yscale('log')\n",
    "    plt.colorbar(label='Magnitude')\n",
    "    plt.title(f'Continuous Wavelet Transform (CWT) - Sensor {sensor_index + 1}')\n",
    "    plt.ylabel('Scale')\n",
    "    plt.xlabel('Time')\n",
    "    plt.show()\n",
    "\n",
    "# # 각 센서에 대한 CWT 결과를 시각화\n",
    "# for i in range(normal_cwt.shape[0]):  # normal_cwt의 첫 번째 차원은 센서의 개수를 나타냄\n",
    "#     visualize_cwt(normal_cwt, i, scales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_cwt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M =15 # 이동평균 필터 사이즈\n",
    "def apply_moving_average(data):\n",
    "    # 이동 평균 적용 및 데이터 재구성\n",
    "    temp = [np.convolve(data[col], np.ones(M), 'valid') / M for col in data.columns]\n",
    "    return np.column_stack(temp)\n",
    "\n",
    "# 이동 평균 필터 적용 -> 노이즈 제거용\n",
    "normal_ma = apply_moving_average(normal_)\n",
    "type1_ma = apply_moving_average(type1_)\n",
    "type2_ma = apply_moving_average(type2_)\n",
    "type3_ma = apply_moving_average(type3_)\n",
    "# CWT 결과와 이동평균 결과 결합\n",
    "print(np.shape(normal_ma),np.shape(normal_cwt))\n",
    "# CWT 결과 중 필요한 부분만 슬라이싱하여 이동 평균 결과와 결합\n",
    "start_index = 14  # 이동 평균을 적용했을 때 데이터가 얼마나 줄어드는지에 따라 조정\n",
    "normal_features = np.concatenate((normal_ma, normal_cwt[start_index:, :]), axis=1)\n",
    "type1_features = np.concatenate((type1_ma, type1_cwt[start_index:, :]), axis=1)\n",
    "type2_features = np.concatenate((type2_ma, type2_cwt[start_index:, :]), axis=1)\n",
    "type3_features = np.concatenate((type3_ma, type3_cwt[start_index:, :]), axis=1)\n",
    "print(np.shape(normal_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(normal_features) # normal_데이터셋의 데이터 분포가 어떻게 정규화되어 있는지 학습\n",
    "\n",
    "# normal_ 데이터셋 분포에 맞게 다른 모든 데이터 셋의 분포를 전환\n",
    "normal= scaler.fit_transform(normal_features)\n",
    "type1 = scaler.transform(type1_features)\n",
    "type2= scaler.transform(type2_features)\n",
    "type3= scaler.transform(type3_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SPLIT ->(Train, Valid, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal)\n",
    "print('------------------------------------------------')\n",
    "print('normal data size = ', normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 끝에 NAN 쓰레기값, 초반에 불안정함때문에 중간 100,000개만 데이터로 사용\n",
    "normal = normal[30000:130000][:]\n",
    "type1 = type1[30000:130000][:]\n",
    "type2 = type2[30000:130000][:]\n",
    "type3 = type3[30000:130000][:]\n",
    "print(normal)\n",
    "print('------------------------------------------------')\n",
    "print('normal data size = ', normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분배, train = 60,000개, valid = 20,000개, test = 20,000개 \n",
    "normal_train = normal[:][:60000]; normal_valid = normal[:][60000:80000]; normal_test =normal[:][80000:]\n",
    "type1_train = type1[:][:60000]; type1_valid = type1[:][60000:80000]; type1_test =type1[:][80000:]\n",
    "type2_train = type2[:][:60000]; type2_valid = type2[:][60000:80000]; type2_test =type2[:][80000:]\n",
    "type3_train = type3[:][:60000]; type3_valid = type3[:][60000:80000]; type3_test =type3[:][80000:]\n",
    "\n",
    "# 데이터 합치기\n",
    "train = np.concatenate((normal_train,type1_train,type2_train,type3_train))\n",
    "valid = np.concatenate((normal_valid,type1_valid,type2_valid,type3_valid))\n",
    "test = np.concatenate((normal_test,type1_test,type2_test,type3_test))\n",
    "print(\"train data의 형태:\", train.shape) # normal_train -> type1_train -> type2_train -> type3_train: 같은 분포\n",
    "print(\"valid data의 형태:\", valid.shape)\n",
    "print(\" test data의 형태:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 예측한 결과값을 담을 데이터 구조 생성\n",
    "train_label = np.concatenate((np.full((60000,1),0), np.full((60000,1),1),\n",
    "np.full((60000,1),2), np.full((60000,1),3)))\n",
    "valid_label = np.concatenate((np.full((20000,1),0), np.full((20000,1),1),\n",
    "np.full((20000,1),2), np.full((20000,1),3)))\n",
    "test_label = np.concatenate((np.full((20000,1),0), np.full((20000,1),1),\n",
    "np.full((20000,1),2), np.full((20000,1),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data, valid data, test data 전부 index 셔플\n",
    "idx = np.arange(train.shape[0]); np.random.shuffle(idx)\n",
    "train = train[:][idx]; train_label = train_label[:][idx]\n",
    "\n",
    "idx_v = np.arange(valid.shape[0]); np.random.shuffle(idx_v)\n",
    "valid = valid[:][idx_v]; valid_label = valid_label[:][idx_v]\n",
    "\n",
    "idx_t = np.arange(test.shape[0]); np.random.shuffle(idx_t)\n",
    "test = test[:][idx_t]; test_label = test_label[:][idx_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토치 텐서로 변환, 그냥 좀 절삭\n",
    "x_train = torch.from_numpy(train).float()\n",
    "y_train = torch.from_numpy(train_label).float().T[0]\n",
    "x_valid = torch.from_numpy(valid).float()\n",
    "y_valid = torch.from_numpy(valid_label).float().T[0]\n",
    "x_test = torch.from_numpy(test).float()\n",
    "y_test = torch.from_numpy(test_label).float().T[0]\n",
    "print(\"변경 전\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"변경 후\")\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# 데이터셋 생성 및 배치사이즈로 미리 나누며 iterator 생성\n",
    "BATCH_SIZE = 4096\n",
    "train = TensorDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(train, batch_size =BATCH_SIZE, shuffle=True)\n",
    "valid = TensorDataset(x_valid, y_valid)\n",
    "valid_dataloader = DataLoader(valid, batch_size =len(x_valid), shuffle=False)\n",
    "test = TensorDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(test, batch_size =len(x_valid), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET AI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAMP_DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAMP_DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features =4, out_features =100)\n",
    "        self.layer2 = nn.Linear(in_features =100, out_features =100)\n",
    "        self.layer3 = nn.Linear(in_features =100, out_features =100)\n",
    "        self.layer4 = nn.Linear(in_features =100, out_features =4)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, input):\n",
    "        out =self.layer1(input)\n",
    "        out =self.relu(out)\n",
    "        out =self.dropout(out)\n",
    "\n",
    "        out =self.layer2(out)\n",
    "        out =self.relu(out)\n",
    "        out =self.dropout(out)\n",
    "\n",
    "        out =self.layer3(out)\n",
    "        out =self.relu(out)\n",
    "        out =self.dropout(out)\n",
    "\n",
    "        out =self.layer4(out)\n",
    "        return out\n",
    "\n",
    "model_check = KAMP_DNN()\n",
    "print(model_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAMP_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAMP_CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=1, out_channels=100, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=100, out_channels=100, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=100, out_channels=100, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=100, out_channels=4, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(4),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1))\n",
    "\n",
    "        self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear = nn.Linear(4, 4)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(1)\n",
    "        out =self.conv1(input)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out =self.conv4(out)\n",
    "        out =self.final_pool(out)\n",
    "        out =self.linear(out.squeeze(-1))\n",
    "        return out\n",
    "model_check = KAMP_CNN()\n",
    "print(model_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------------------------------------------------------')\n",
    "input = torch.tensor([[[0.0, 6.0, 9.0, 1.0]]])\n",
    "print('\"input is same below.\"')\n",
    "print(input)\n",
    "print('--------------------------------------------------------------------')\n",
    "model = nn.Conv1d(1, 4, 2, bias =False)\n",
    "model.weight.data = torch.zeros(model.weight.data.size())\n",
    "model.weight.data[:, :, :2] =1\n",
    "print('\"kernal is same below.\"')\n",
    "print(model.weight.data)\n",
    "print('--------------------------------------------------------------------')\n",
    "output = model(input)\n",
    "print('\"output is same below (without bias).\"')\n",
    "print(output)\n",
    "print('--------------------------------------------------------------------')\n",
    "model1 = nn.Conv1d(1, 4, 2)\n",
    "model1.weight.data = torch.zeros(model1.weight.data.size())\n",
    "model1.weight.data[:, :, :2] =1\n",
    "output = model1(input)\n",
    "print('\"output is same below (with bias).\"')\n",
    "print(output)\n",
    "print('--------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class KAMP_RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAMP_RNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size =4, hidden_size =100, num_layers =2,\n",
    "        batch_first=True, dropout =0.2)\n",
    "        self.fc = nn.Linear(in_features =100, out_features =4)\n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(1)\n",
    "        out, _ =self.lstm(input)\n",
    "        out = out.view(-1,100)\n",
    "        output =self.fc(out)\n",
    "        return output\n",
    "\n",
    "model_check = KAMP_RNN()\n",
    "print(model_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU: device\n",
    "def train_model(model, criterion, optimizer, num_epoch, train_dataloader, PATH):\n",
    " # Model을 GPU로 이동\n",
    " model.to(device)\n",
    "\n",
    " \n",
    " loss_values = []\n",
    " loss_values_v = [] \n",
    " accuracy_past =0\n",
    " for epoch in range(1, num_epoch +1):\n",
    "    #---------------------- 모델 학습 ---------------------#\n",
    "    model.train()\n",
    "    batch_number =0\n",
    "    running_loss =0.0\n",
    "    for batch_idx, samples in enumerate(train_dataloader):\n",
    "        # 데이터 GPU로 옮기기\n",
    "        x_train, y_train = samples[0].to(device), samples[1].to(device) \n",
    "\n",
    "        # 변수 초기화\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model.forward(x_train)\n",
    "        loss = criterion(y_hat,y_train.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        batch_number +=1\n",
    "\n",
    "    loss_values.append(running_loss / batch_number)\n",
    " #---------------------- 모델 검증 ---------------------#\n",
    "    model.eval()\n",
    "    accuracy =0.0\n",
    "    total =0.0\n",
    "    for batch_idx, data in enumerate(valid_dataloader):\n",
    "        x_valid, y_valid = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        v_hat = model.forward(x_valid)\n",
    "        v_loss = criterion(v_hat,y_valid.long())\n",
    "        _, predicted = torch.max(v_hat.data, 1)\n",
    "        total += y_valid.size(0)\n",
    "        accuracy += (predicted == y_valid).sum().item()\n",
    "    loss_values_v.append(loss.item())\n",
    "    accuracy = (accuracy / total)\n",
    " #----------------Check for early stopping---------------#\n",
    "    if epoch % 1 ==0:\n",
    "        print('[Epoch {}/{}] [Train_Loss: {:.6f} /Valid_Loss: {:.6f}]'.format(epoch, num_epochs, loss.item(),v_loss.item()))\n",
    "        print('[Epoch {}/{}] [Accuracy : {:.6f}]'.format(epoch, num_epochs, accuracy))\n",
    "    \n",
    "    # checkpoint + early stopping\n",
    "    if accuracy_past < accuracy:\n",
    "        accuracy_past = accuracy\n",
    "        torch.save(model.state_dict(), PATH + f'model_epoch_{epoch}_acc_{accuracy:.4f}.pt')\n",
    "        print(f\"Checkpoint saved at epoch {epoch} with validation accuracy {accuracy:.4f}.\")\n",
    "\n",
    "# return loss..\n",
    " return loss_values, loss_values_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model = KAMP_CNN()\n",
    "num_epochs =1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(CNN_model.parameters())\n",
    "PATH ='save/CNN/'\n",
    "CNN_loss_values, CNN_loss_values_v = train_model(CNN_model, criterion, optimizer,\n",
    "num_epochs, train_dataloader, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = KAMP_RNN()\n",
    "num_epochs =1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(RNN_model.parameters())\n",
    "PATH ='save/RNN/'\n",
    "RNN_loss_values, RNN_loss_values_v = train_model(RNN_model, criterion, optimizer,\n",
    "num_epochs, train_dataloader, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, PATH):\n",
    "    model = torch.load(PATH +'model.pt')\n",
    "    #---------------------- 모델 시험 ---------------------#\n",
    "    model.eval()\n",
    "    total =0.0\n",
    "    accuracy =0.0\n",
    "    for batch_idx, data in enumerate(test_dataloader):\n",
    "        x_test, y_test = data[0].to(device),data[1].to(device)\n",
    "\n",
    "        t_hat = model(x_test)\n",
    "        _, predicted = torch.max(t_hat.data, 1)\n",
    "        total += y_test.size(0)\n",
    "        accuracy += (predicted == y_test).sum().item()\n",
    "    accuracy = (accuracy / total)\n",
    "    #------------------------------------------------------#\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def draw_confusion_matrix(model, xt, yt, PATH):\n",
    "    y_pred = []; y_true = []\n",
    "    # 1. 모델 예측 결과 뽑기\n",
    "    model.eval()\n",
    "    y_hat = model(xt) # y_hat은 모델로 예측한 결과\n",
    "    output = (torch.max(torch.exp(y_hat), 1)[1]).data.cpu().numpy() # 결과값만 추출 \n",
    "    y_pred.extend(output)\n",
    "\n",
    "    # 2. 실제 정답값 뽑기\n",
    "    labels = y_test.data.cpu().numpy()\n",
    "    y_true.extend(labels)\n",
    "    # 분류 항목\n",
    "    classes = ('Normal', 'Type1', 'Type2', 'Type3')\n",
    "\n",
    "    # Confussion Matrix 생성\n",
    "    plt.figure(figsize = (7,5))\n",
    "    dlen = float(len(x_test)) # test data 크기 : 여기서는 80000\n",
    "    cm = confusion_matrix(y_true, y_pred) \n",
    "\n",
    "    df_cm = pd.DataFrame(cm/dlen, index = [i for i in classes],columns = [i for i in classes])\n",
    "    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\", size=24, fontweight='bold')\n",
    "    plt.xlabel(\"Predicted Label\", size=16); plt.ylabel(\"Actual Label\", size=16)\n",
    "    plt.rc('xtick', labelsize=12); plt.rc('ytick', labelsize=12); plt.yticks(rotation=0)\n",
    "    plt.savefig(PATH +'cm_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_graph(loss_values, loss_values_v):\n",
    "    plt.figure()\n",
    "    plt.plot(loss_values)\n",
    "    plt.plot(loss_values_v)\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.ylabel(\"loss\", fontsize=\"large\")\n",
    "    plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "    plt.legend([\"train\", \"validation\"])\n",
    "    plt.tight_layout()\n",
    "    # 결과 저장\n",
    "    plt.savefig(PATH +'lossplot_output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(CNN_model,'save/CNN/')\n",
    "# test_model(RNN_model,'save/RNN/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confusion_matrix(CNN_model, x_test, y_test, 'save/CNN/')\n",
    "draw_confusion_matrix(RNN_model, x_test, y_test, 'save/RNN/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_graph(CNN_loss_values, CNN_loss_values_v)\n",
    "plot_loss_graph(RNN_loss_values, RNN_loss_values_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-job",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

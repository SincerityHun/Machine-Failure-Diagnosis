{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install seaborn\n",
    "!pip install torch\n",
    "!pip install matplotlib\n",
    "!pip install scipy\n",
    "!pip install scikit-learn\n",
    "!pip install pyWavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from scipy.fft import fft, ifft\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU SETTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # 사용 가능한 GPU의 개수\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of GPUs available: {num_gpus}\")\n",
    "\n",
    "    # Device 세팅\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available and set as the current device.\")\n",
    "\n",
    "    # 현재 device로 설정된 GPU 확인\n",
    "    current_device = torch.cuda.current_device()\n",
    "    print(f\"Current GPU Device: index[{current_device}]\")\n",
    "    \n",
    "    # 현재 device로 설정된 GPU의 이름 출력\n",
    "    print(f\"Device name: {torch.cuda.get_device_name(current_device)}\")\n",
    "    \n",
    "    \n",
    "else:\n",
    "    print(\"GPU is not available, using CPU instead.\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SETTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor1 = pd.read_csv(\"raw_data/g1_sensor1.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])\n",
    "sensor2 = pd.read_csv(\"raw_data/g1_sensor2.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])\n",
    "sensor3 = pd.read_csv(\"raw_data/g1_sensor3.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])\n",
    "sensor4 = pd.read_csv(\"raw_data/g1_sensor4.csv\",names=[\"time\",\"normal\",\"type1\",\"type2\",\"type3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sensor 1의 데이터 크기\",sensor1.shape) # 190218개의 행, 5의 열\n",
    "print(\"sensor 1의 데이터 크기\",sensor2.shape)\n",
    "print(\"sensor 1의 데이터 크기\",sensor3.shape)\n",
    "print(\"sensor 1의 데이터 크기\",sensor4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import interpolate\n",
    "x_new = np.arange(0,140,0.001) # 0,0.001,0.002,,,,,,139.999\n",
    "y_new1 = []\n",
    "y_new2 = []\n",
    "y_new3 = []\n",
    "y_new4 = []\n",
    "\n",
    "# 모든 센서의 각 타입 데이터 별로 선형 보간을 수행한 결과를 추출\n",
    "for item in [\"normal\",\"type1\",\"type2\",\"type3\"]:\n",
    "    f_linear1 = interpolate.interp1d(sensor1[\"time\"],sensor1[item],kind=\"linear\") \n",
    "    y_new1.append(f_linear1(x_new)) \n",
    "\n",
    "    f_linear2 = interpolate.interp1d(sensor2[\"time\"],sensor2[item],kind=\"linear\")\n",
    "    y_new2.append(f_linear2(x_new))\n",
    "    f_linear3 = interpolate.interp1d(sensor3[\"time\"],sensor3[item],kind=\"linear\")\n",
    "    y_new3.append(f_linear3(x_new))\n",
    "    f_linear4 = interpolate.interp1d(sensor4[\"time\"],sensor4[item],kind=\"linear\")\n",
    "    y_new4.append(f_linear4(x_new))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간축을 기준으로, 모든 센서에서 추출된 데이터를 이어붙인다.\n",
    "normal_ = pd.concat([sensor1[\"normal\"],sensor2[\"normal\"],sensor3[\"normal\"],sensor4[\"normal\"]],axis=1) # 각 센서에서 추출된 normal 데이터 \n",
    "type1_ = pd.concat([sensor1[\"type1\"],sensor2[\"type1\"],sensor3[\"type1\"],sensor4[\"type1\"]],axis=1) # Type 1 이상치 데이터\n",
    "type2_ = pd.concat([sensor1[\"type2\"],sensor2[\"type2\"],sensor3[\"type2\"],sensor4[\"type2\"]],axis=1) # Type 2 이상치 데이터\n",
    "type3_ = pd.concat([sensor1[\"type3\"],sensor2[\"type3\"],sensor3[\"type3\"],sensor4[\"type3\"]],axis=1) # Type 3 이상치 데이터\n",
    "\n",
    "# 어디 센서에서 나온 결과인지, 열의 이름 달기\n",
    "normal_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
    "type1_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
    "type2_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]\n",
    "type3_.columns = [\"s1\",\"s2\",\"s3\",\"s4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기본적인 분포\n",
    "    * 주파수가 현재 normal, type1, type2, type3가 달라서, 정규 분포가 다름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,4))\n",
    "plt.scatter(range(0,300),normal_[\"s1\"][:300],label=\"class=\"+str(1),marker='o',s =5) # x축: (0,300), y축: normal_[\"s1\"][:300]\n",
    "# plt.scatter(range(0,300),type1_[\"s1\"][:300],label=\"class=\"+str(2),marker='o',s =5)\n",
    "# plt.scatter(range(0,300),type2_[\"s1\"][:300],label=\"class=\"+str(3),marker='o',s =5)\n",
    "# plt.scatter(range(0,300),type3_[\"s1\"][:300],label=\"class=\"+str(4),marker='o',s =5)\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Sensor\",fontsize=15)\n",
    "plt.ylabel(\"Sensor Value\",fontsize=15)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주어진 주파수를 Morlet 웨이블릿을 활용해 CWT변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 웨이블릿 생성 코드\n",
    "def psi(T, f0=6):\n",
    "    '''\n",
    "\t  T : parameter for adjusting length of wavelet\n",
    "\t  f0 : parameter for time-frequenct resolution trade off \n",
    "    '''\n",
    "    x = np.linspace(-2 * np.pi, 2 * np.pi, T)\n",
    "    return (np.pi ** -0.25) * np.exp(1j * f0 * x - x ** 2 / 2)\n",
    "\n",
    "# 특정 웨이블릿과 주어진 신호 사이의 상관계수 Convolution 연산 코드\n",
    "def wavelet_convolution(tup):\n",
    "    f = tup[0]\n",
    "    T = tup[1]\n",
    "    f_len = np.shape(f)[0]\n",
    "    f_hat = np.append(f, np.zeros(T))\n",
    "    h = psi(T)\n",
    "    h_hat = np.append(h, np.zeros(f_len))\n",
    "    return ifft(fft(f_hat)*fft(h_hat))[round(T/2) : round(T/2) + f_len]\n",
    "\n",
    "# 전체 시간의 CWT 출력\n",
    "def cwt(f, t0 = 20):\n",
    "    '''\n",
    "    f : input signal\n",
    "    t0 : minimum length of wavelet\n",
    "    '''\n",
    "    f_len = np.shape(f)[0]\n",
    "    result = np.array(list(map(wavelet_convolution, [(f, x) for x in range(t0, f_len, 10)])))\n",
    "    return result\n",
    "\n",
    "# 기본 Input 시그널 확인\n",
    "input_signal = normal_[\"s1\"]\n",
    "print(np.shape(input_signal))\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(range(0, 300), input_signal[:300], label=\"class=\" + str(1))\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리를 활용한 CWT 변환\n",
    "wavelet = 'cmor'\n",
    "scales = np.arange(1,64) # 스케일이 커질 수록, 낮은 주파수 성분을 잡아낼 수 있음\n",
    "coefficients, frequencies = pywt.cwt(input_signal, scales, wavelet)\n",
    "# CWT 결과를 시각화합니다.\n",
    "print(np.abs(coefficients))\n",
    "plt.imshow(np.abs(coefficients), extent=[0, len(input_signal), frequencies[-1], frequencies[0]], aspect='auto')\n",
    "plt.yscale('log')\n",
    "plt.colorbar(label='Magnitude')\n",
    "plt.title('Continuous Wavelet Transform (CWT)')\n",
    "plt.ylabel('Frequency (Hz)')\n",
    "plt.xlabel('Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CWT 변환 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply_cwt 함수 정의: 각 센서 데이터에 대해 CWT를 적용합니다.\n",
    "def apply_cwt(data, scales, wavelet='cmor'):\n",
    "    cwt_coeffs = []\n",
    "    for column in data:\n",
    "        sensor_data = data[column].values  # Pandas Series를 numpy 배열로 변환\n",
    "        if sensor_data.ndim != 1:\n",
    "            raise ValueError(f\"Data for sensor {column} is not 1-dimensional.\")\n",
    "        # 연속 웨이블릿 변환 적용\n",
    "        cwt_matrix, frequencies = pywt.cwt(sensor_data, scales, wavelet)\n",
    "        # 해당 시계열의 cwt_matrix의 절댓값이 가장 큰 값의 frequencies을 넣어준다.\n",
    "        # Find the index of the max coefficient at each time point across all scales\n",
    "        cwt_coeffs.append(np.max(np.abs(cwt_matrix),axis=0))  # 결과를 1차원으로 평탄\n",
    "        # cwt_coeffs.append(np.abs(cwt_matrix))  # 결과를 1차원으로 평탄\n",
    "    # numpy 배열로 변환\n",
    "    return np.column_stack(cwt_coeffs)\n",
    "\n",
    "wavelet = 'morl'\n",
    "scales = np.arange(1,128) # 스케일이 커질 수록, 낮은 주파수 성분을 잡아낼 수 있음-> 이거 그냥 128이 제일 잘 나와서 이걸로 함\n",
    "\n",
    "# 각 데이터셋에 대해 CWT 변환 적용\n",
    "normal_cwt = apply_cwt(normal_, scales,wavelet)\n",
    "type1_cwt = apply_cwt(type1_, scales,wavelet)\n",
    "type2_cwt = apply_cwt(type2_, scales,wavelet)\n",
    "type3_cwt = apply_cwt(type3_, scales,wavelet)\n",
    "\n",
    "# CWT 결과를 시각화하는 함수 정의\n",
    "def visualize_cwt(coefficients, sensor_index, scales):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(np.abs(coefficients[sensor_index]), extent=[0, len(coefficients[sensor_index][0]), scales[-1], scales[0]], aspect='auto', cmap='viridis')\n",
    "    plt.yscale('log')\n",
    "    plt.colorbar(label='Magnitude')\n",
    "    plt.title(f'Continuous Wavelet Transform (CWT) - Sensor {sensor_index + 1}')\n",
    "    plt.ylabel('Scale')\n",
    "    plt.xlabel('Time')\n",
    "    plt.show()\n",
    "\n",
    "# # 각 센서에 대한 CWT 결과를 시각화\n",
    "# for i in range(normal_cwt.shape[0]):  # normal_cwt의 첫 번째 차원은 센서의 개수를 나타냄\n",
    "#     visualize_cwt(normal_cwt, i, scales)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal_cwt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M =15 # 이동평균 필터 사이즈\n",
    "def apply_moving_average(data):\n",
    "    # 이동 평균 적용 및 데이터 재구성\n",
    "    temp = [np.convolve(data[col], np.ones(M), 'valid') / M for col in data.columns]\n",
    "    return np.column_stack(temp)\n",
    "\n",
    "# 이동 평균 필터 적용 -> 노이즈 제거용\n",
    "normal_ma = apply_moving_average(normal_)\n",
    "type1_ma = apply_moving_average(type1_)\n",
    "type2_ma = apply_moving_average(type2_)\n",
    "type3_ma = apply_moving_average(type3_)\n",
    "# CWT 결과와 이동평균 결과 결합\n",
    "print(np.shape(normal_ma),np.shape(normal_cwt))\n",
    "# CWT 결과 중 필요한 부분만 슬라이싱하여 이동 평균 결과와 결합\n",
    "start_index = 14  # 이동 평균을 적용했을 때 데이터가 얼마나 줄어드는지에 따라 조정\n",
    "normal_features = np.concatenate((normal_ma, normal_cwt[start_index:, :]), axis=1)\n",
    "type1_features = np.concatenate((type1_ma, type1_cwt[start_index:, :]), axis=1)\n",
    "type2_features = np.concatenate((type2_ma, type2_cwt[start_index:, :]), axis=1)\n",
    "type3_features = np.concatenate((type3_ma, type3_cwt[start_index:, :]), axis=1)\n",
    "print(np.shape(normal_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(normal_features) # normal_데이터셋의 데이터 분포가 어떻게 정규화되어 있는지 학습\n",
    "\n",
    "# normal_ 데이터셋 분포에 맞게 다른 모든 데이터 셋의 분포를 전환\n",
    "normal= scaler.fit_transform(normal_features)\n",
    "type1 = scaler.transform(type1_features)\n",
    "type2= scaler.transform(type2_features)\n",
    "type3= scaler.transform(type3_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA SPLIT ->(Train, Valid, Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normal)\n",
    "print('------------------------------------------------')\n",
    "print('normal data size = ', normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 끝에 NAN 쓰레기값, 초반에 불안정함때문에 중간 100,000개만 데이터로 사용\n",
    "normal = normal[30000:130000][:]\n",
    "type1 = type1[30000:130000][:]\n",
    "type2 = type2[30000:130000][:]\n",
    "type3 = type3[30000:130000][:]\n",
    "print(normal)\n",
    "print('------------------------------------------------')\n",
    "print('normal data size = ', normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 분배, train = 60,000개, valid = 20,000개, test = 20,000개 \n",
    "normal_train = normal[:][:60000]; normal_valid = normal[:][60000:80000]; normal_test =normal[:][80000:]\n",
    "type1_train = type1[:][:60000]; type1_valid = type1[:][60000:80000]; type1_test =type1[:][80000:]\n",
    "type2_train = type2[:][:60000]; type2_valid = type2[:][60000:80000]; type2_test =type2[:][80000:]\n",
    "type3_train = type3[:][:60000]; type3_valid = type3[:][60000:80000]; type3_test =type3[:][80000:]\n",
    "\n",
    "# 데이터 합치기\n",
    "train = np.concatenate((normal_train,type1_train,type2_train,type3_train))\n",
    "valid = np.concatenate((normal_valid,type1_valid,type2_valid,type3_valid))\n",
    "test = np.concatenate((normal_test,type1_test,type2_test,type3_test))\n",
    "print(\"train data의 형태:\", train.shape) # normal_train -> type1_train -> type2_train -> type3_train: 같은 분포\n",
    "print(\"valid data의 형태:\", valid.shape)\n",
    "print(\" test data의 형태:\", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델이 예측한 결과값을 담을 데이터 구조 생성\n",
    "train_label = np.concatenate((np.full((60000,1),0), np.full((60000,1),1),\n",
    "np.full((60000,1),2), np.full((60000,1),3)))\n",
    "valid_label = np.concatenate((np.full((20000,1),0), np.full((20000,1),1),\n",
    "np.full((20000,1),2), np.full((20000,1),3)))\n",
    "test_label = np.concatenate((np.full((20000,1),0), np.full((20000,1),1),\n",
    "np.full((20000,1),2), np.full((20000,1),3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data, valid data, test data 전부 index 셔플\n",
    "idx = np.arange(train.shape[0]); np.random.shuffle(idx)\n",
    "train = train[:][idx]; train_label = train_label[:][idx]\n",
    "\n",
    "idx_v = np.arange(valid.shape[0]); np.random.shuffle(idx_v)\n",
    "valid = valid[:][idx_v]; valid_label = valid_label[:][idx_v]\n",
    "\n",
    "idx_t = np.arange(test.shape[0]); np.random.shuffle(idx_t)\n",
    "test = test[:][idx_t]; test_label = test_label[:][idx_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토치 텐서로 변환, 그냥 좀 절삭\n",
    "x_train = torch.from_numpy(train).float()\n",
    "y_train = torch.from_numpy(train_label).float().T[0]\n",
    "x_valid = torch.from_numpy(valid).float()\n",
    "y_valid = torch.from_numpy(valid_label).float().T[0]\n",
    "x_test = torch.from_numpy(test).float()\n",
    "y_test = torch.from_numpy(test_label).float().T[0]\n",
    "print(\"변경 전\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"변경 후\")\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "# 데이터셋 생성 및 배치사이즈로 미리 나누며 iterator 생성\n",
    "BATCH_SIZE = 5000\n",
    "train = TensorDataset(x_train, y_train)\n",
    "train_dataloader = DataLoader(train, batch_size =BATCH_SIZE, shuffle=True)\n",
    "valid = TensorDataset(x_valid, y_valid)\n",
    "valid_dataloader = DataLoader(valid, batch_size =len(x_valid), shuffle=False)\n",
    "test = TensorDataset(x_test, y_test)\n",
    "test_dataloader = DataLoader(test, batch_size =len(x_valid), shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SET AI MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAMP_DNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAMP_DNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_features =4, out_features =100)\n",
    "        self.layer2 = nn.Linear(in_features =100, out_features =100)\n",
    "        self.layer3 = nn.Linear(in_features =100, out_features =100)\n",
    "        self.layer4 = nn.Linear(in_features =100, out_features =4)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, input):\n",
    "        out =self.layer1(input)\n",
    "        out =self.relu(out)\n",
    "        out =self.dropout(out)\n",
    "\n",
    "        out =self.layer2(out)\n",
    "        out =self.relu(out)\n",
    "        out =self.dropout(out)\n",
    "\n",
    "        out =self.layer3(out)\n",
    "        out =self.relu(out)\n",
    "        out =self.dropout(out)\n",
    "\n",
    "        out =self.layer4(out)\n",
    "        return out\n",
    "\n",
    "model_check = KAMP_DNN()\n",
    "print(model_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KAMP_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAMP_CNN, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=1, out_channels=100, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=100, out_channels=100, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=100, out_channels=100, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(100),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1),\n",
    "        nn.Dropout(p=0.2))\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "        nn.Conv1d(in_channels=100, out_channels=4, kernel_size=2, stride=1, padding='same'),\n",
    "        nn.BatchNorm1d(4),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool1d(kernel_size=1, stride=1))\n",
    "\n",
    "        self.final_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.linear = nn.Linear(4, 4)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(1)\n",
    "        out =self.conv1(input)\n",
    "        # out = self.conv2(out)\n",
    "        # out = self.conv3(out)\n",
    "        out =self.conv4(out)\n",
    "        out =self.final_pool(out)\n",
    "        out =self.linear(out.squeeze(-1))\n",
    "        return out\n",
    "model_check = KAMP_CNN()\n",
    "print(model_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--------------------------------------------------------------------')\n",
    "input = torch.tensor([[[0.0, 6.0, 9.0, 1.0]]])\n",
    "print('\"input is same below.\"')\n",
    "print(input)\n",
    "print('--------------------------------------------------------------------')\n",
    "model = nn.Conv1d(1, 4, 2, bias =False)\n",
    "model.weight.data = torch.zeros(model.weight.data.size())\n",
    "model.weight.data[:, :, :2] =1\n",
    "print('\"kernal is same below.\"')\n",
    "print(model.weight.data)\n",
    "print('--------------------------------------------------------------------')\n",
    "output = model(input)\n",
    "print('\"output is same below (without bias).\"')\n",
    "print(output)\n",
    "print('--------------------------------------------------------------------')\n",
    "model1 = nn.Conv1d(1, 4, 2)\n",
    "model1.weight.data = torch.zeros(model1.weight.data.size())\n",
    "model1.weight.data[:, :, :2] =1\n",
    "output = model1(input)\n",
    "print('\"output is same below (with bias).\"')\n",
    "print(output)\n",
    "print('--------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class KAMP_RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(KAMP_RNN, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size =4, hidden_size =100, num_layers =2,\n",
    "        batch_first=True, dropout =0.2)\n",
    "        self.fc = nn.Linear(in_features =100, out_features =4)\n",
    "    def forward(self, input):\n",
    "        input = input.unsqueeze(1)\n",
    "        out, _ =self.lstm(input)\n",
    "        out = out.view(-1,100)\n",
    "        output =self.fc(out)\n",
    "        return output\n",
    "\n",
    "model_check = KAMP_RNN()\n",
    "print(model_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU: device\n",
    "def train_model(model, criterion, optimizer, num_epoch, train_dataloader, PATH):\n",
    " # Model을 GPU로 이동\n",
    " model.to(device)\n",
    "\n",
    " \n",
    " loss_values = []\n",
    " loss_values_v = [] \n",
    " accuracy_past =0\n",
    " for epoch in range(1, num_epoch +1):\n",
    "    #---------------------- 모델 학습 ---------------------#\n",
    "    model.train()\n",
    "    batch_number =0\n",
    "    running_loss =0.0\n",
    "    for batch_idx, samples in enumerate(train_dataloader):\n",
    "        # 데이터 GPU로 옮기기\n",
    "        x_train, y_train = samples[0].to(device), samples[1].to(device) \n",
    "\n",
    "        # 변수 초기화\n",
    "        optimizer.zero_grad()\n",
    "        y_hat = model.forward(x_train)\n",
    "        loss = criterion(y_hat,y_train.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        batch_number +=1\n",
    "\n",
    "    loss_values.append(running_loss / batch_number)\n",
    " #---------------------- 모델 검증 ---------------------#\n",
    "    model.eval()\n",
    "    accuracy =0.0\n",
    "    total =0.0\n",
    "    for batch_idx, data in enumerate(valid_dataloader):\n",
    "        x_valid, y_valid = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        v_hat = model.forward(x_valid)\n",
    "        v_loss = criterion(v_hat,y_valid.long())\n",
    "        _, predicted = torch.max(v_hat.data, 1)\n",
    "        total += y_valid.size(0)\n",
    "        accuracy += (predicted == y_valid).sum().item()\n",
    "    loss_values_v.append(loss.item())\n",
    "    accuracy = (accuracy / total)\n",
    " #----------------Check for early stopping---------------#\n",
    "    if epoch % 1 ==0:\n",
    "        print('[Epoch {}/{}] [Train_Loss: {:.6f} /Valid_Loss: {:.6f}]'.format(epoch, num_epochs, loss.item(),v_loss.item()))\n",
    "        print('[Epoch {}/{}] [Accuracy : {:.6f}]'.format(epoch, num_epochs, accuracy))\n",
    "    \n",
    "    # checkpoint + early stopping\n",
    "    if accuracy_past < accuracy:\n",
    "        accuracy_past = accuracy\n",
    "        torch.save(model.state_dict(), PATH + f'model_epoch_{epoch}_acc_{accuracy:.4f}.pt')\n",
    "        print(f\"Checkpoint saved at epoch {epoch} with validation accuracy {accuracy:.4f}.\")\n",
    "\n",
    "# return loss..\n",
    " return loss_values, loss_values_v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model = KAMP_CNN()\n",
    "num_epochs =1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(CNN_model.parameters())\n",
    "PATH ='save/CNN/'\n",
    "CNN_loss_values, CNN_loss_values_v = train_model(CNN_model, criterion, optimizer,\n",
    "num_epochs, train_dataloader, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNN_model = KAMP_RNN()\n",
    "num_epochs =1000\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(RNN_model.parameters())\n",
    "PATH ='save/RNN/'\n",
    "RNN_loss_values, RNN_loss_values_v = train_model(RNN_model, criterion, optimizer,\n",
    "num_epochs, train_dataloader, PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, PATH):\n",
    "    model = torch.load(PATH +'model.pt')\n",
    "    #---------------------- 모델 시험 ---------------------#\n",
    "    model.eval()\n",
    "    total =0.0\n",
    "    accuracy =0.0\n",
    "    for batch_idx, data in enumerate(test_dataloader):\n",
    "        x_test, y_test = data[0].to(device),data[1].to(device)\n",
    "\n",
    "        t_hat = model(x_test)\n",
    "        _, predicted = torch.max(t_hat.data, 1)\n",
    "        total += y_test.size(0)\n",
    "        accuracy += (predicted == y_test).sum().item()\n",
    "    accuracy = (accuracy / total)\n",
    "    #------------------------------------------------------#\n",
    "    print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def draw_confusion_matrix(model, xt, yt, PATH):\n",
    "    y_pred = []; y_true = []\n",
    "    # 1. 모델 예측 결과 뽑기\n",
    "    model.eval()\n",
    "    y_hat = model(xt) # y_hat은 모델로 예측한 결과\n",
    "    output = (torch.max(torch.exp(y_hat), 1)[1]).data.cpu().numpy() # 결과값만 추출 \n",
    "    y_pred.extend(output)\n",
    "\n",
    "    # 2. 실제 정답값 뽑기\n",
    "    labels = y_test.data.cpu().numpy()\n",
    "    y_true.extend(labels)\n",
    "    # 분류 항목\n",
    "    classes = ('Normal', 'Type1', 'Type2', 'Type3')\n",
    "\n",
    "    # Confussion Matrix 생성\n",
    "    plt.figure(figsize = (7,5))\n",
    "    dlen = float(len(x_test)) # test data 크기 : 여기서는 80000\n",
    "    cm = confusion_matrix(y_true, y_pred) \n",
    "\n",
    "    df_cm = pd.DataFrame(cm/dlen, index = [i for i in classes],columns = [i for i in classes])\n",
    "    sns.heatmap(df_cm, annot=True, cmap=plt.cm.Blues)\n",
    "    plt.title(\"Confusion Matrix\", size=24, fontweight='bold')\n",
    "    plt.xlabel(\"Predicted Label\", size=16); plt.ylabel(\"Actual Label\", size=16)\n",
    "    plt.rc('xtick', labelsize=12); plt.rc('ytick', labelsize=12); plt.yticks(rotation=0)\n",
    "    plt.savefig(PATH +'cm_output.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_graph(loss_values, loss_values_v):\n",
    "    plt.figure()\n",
    "    plt.plot(loss_values)\n",
    "    plt.plot(loss_values_v)\n",
    "    plt.title(\"Training & Validation Loss\")\n",
    "    plt.ylabel(\"loss\", fontsize=\"large\")\n",
    "    plt.xlabel(\"epoch\", fontsize=\"large\")\n",
    "    plt.legend([\"train\", \"validation\"])\n",
    "    plt.tight_layout()\n",
    "    # 결과 저장\n",
    "    plt.savefig(PATH +'lossplot_output.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(CNN_model,'save/CNN/')\n",
    "# test_model(RNN_model,'save/RNN/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_confusion_matrix(CNN_model, x_test, y_test, 'save/CNN/')\n",
    "draw_confusion_matrix(RNN_model, x_test, y_test, 'save/RNN/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss_graph(CNN_loss_values, CNN_loss_values_v)\n",
    "plot_loss_graph(RNN_loss_values, RNN_loss_values_v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-job",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
